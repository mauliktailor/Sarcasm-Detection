# Tokenization-on-different-datasets-NLP-

In this repository, I have performed the basic preprocessing task on any text dataset.

Tokens are the building blocks of Natural Language. Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types â€“ word, character, and subword (n-gram characters) tokenization.

In this repository I have performed the word level tokenization using Keras in TensorFlow 2.0 by using Tokenizer and pad_sequences in Keras.
